Website Crawler â€“ Sykell Test Task
ğŸ“– Overview

A full-stack web application for crawling websites and visualizing crawl results.

Features include:

    Add URLs for crawling

    View crawl results with charts and detailed info

    Perform bulk actions

    Real-time status updates

    Responsive UI

ğŸ›  Tech Stack

    Frontend: React, TypeScript, React Router, React Testing Library

    Backend: Go (Gin framework), MySQL

    Authentication: JWT tokens

ğŸš€ Setup Instructions
Backend

    Install dependencies:

        Go 1.20+

        MySQL 8+

    Create a MySQL database.

    Create a .env file in the root with the following content:

DB_USER=root
DB_PASSWORD=yourpassword
DB_NAME=crawler


Run database migrations:

go run cmd/migrate/main.go

Start the backend server:

    go run cmd/server/main.go

    Backend will be available at http://localhost:4000

Frontend

    Install Node.js (version 18+)

    Install dependencies:

npm install

Start the dev server:

    npm start

    Frontend will run at http://localhost:3000

ğŸ§ª Running Tests

To run frontend tests:

npm test

    Tests cover key flows: adding URLs, starting crawls, viewing results.

ğŸ” API Summary

All requests require an Authorization header:

Authorization: Bearer <token>

Main Endpoints:

    POST /api/urls â€“ Add a new URL

    GET /api/urls â€“ List all URLs with crawl results

    POST /api/urls/:id/start â€“ Start crawling a URL

    POST /api/urls/:id/stop â€“ Stop crawling a URL

    DELETE /api/urls/:id â€“ Delete a URL

âœ¨ Features

    âœ… Add, start, stop, and delete URLs

    âœ… Paginated and filterable results table

    âœ… Detailed view with charts and broken links

    âœ… Bulk re-crawl and deletion

    âœ… Real-time crawling status (polling)

    âœ… Mobile-friendly UI

